"""
Pydantic-ready JSON tool dispatcher for LM Studio (OpenAI-compatible tools)
- Tools: read_json, find_in_json, write_to_json, update_in_json
- Storage: single JSON file with collections as lists (e.g., {"customers": [...], "orders": [...]})
- Validates tool arguments with Pydantic (v1 or v2), forbids extra fields, and supplies defaults
- Auto-repair loop: feeds structured validation errors back to the model for self-correction

Usage
-----
1) pip install openai pydantic
2) Start LM Studio local server (Default: http://127.0.0.1:1234/v1)
3) Run this script. It demonstrates a simple conversation that triggers tool calls.

Forward-looking
---------------
- You can swap the storage layer with SQLite/DuckDB without changing the tool API.
- Keep the same Pydantic models to preserve validation and auto-repair behavior.
"""

from __future__ import annotations
import json, os, tempfile, re
from typing import Any, Dict, List, Optional, Literal

# --- OpenAI-compatible client (LM Studio) ---
try:
    from openai import OpenAI
except Exception as e:
    raise SystemExit("Please `pip install openai` (>=1.0).")

# Point this at LM Studio local server
LM_BASE_URL = os.environ.get("LM_BASE_URL", "http://127.0.0.1:1234/v1")
LM_API_KEY  = os.environ.get("LM_API_KEY", "lm-studio")  # token is ignored by LM Studio, but required by SDK
MODEL       = os.environ.get("LM_MODEL", "lmstudio-community/qwen2.5-7b-instruct")

client = OpenAI(base_url=LM_BASE_URL, api_key=LM_API_KEY)

# --- Storage config ---
STORE_PATH = os.environ.get("JSON_STORE", "store.json")
IDEMPOTENCY_LOG = os.path.splitext(STORE_PATH)[0] + ".idempotency.log"

# --- Tools schema (for the chat.completions.create call) ---
tools = [
  {
    "type": "function",
    "function": {
      "name": "read_json",
      "description": "Read the whole JSON, a collection, an item by ID, or a dot-path (e.g., 'settings.theme').",
      "parameters": {
        "type": "object",
        "properties": {
          "collection": {"type": "string", "pattern": "^[a-zA-Z0-9_-]+$"},
          "id": {"oneOf": [{"type":"integer"}, {"type":"string"}]},
          "path": {"type": "string", "description": "Dot-path, e.g., 'settings.theme' or 'customers.0.email'."}
        },
        "additionalProperties": False
      }
    }
  },
  {
    "type": "function",
    "function": {
      "name": "find_in_json",
      "description": "Filter items in a collection with simple conditions, sorting, and pagination.",
      "parameters": {
        "type": "object",
        "properties": {
          "collection": {"type": "string", "pattern": "^[a-zA-Z0-9_-]+$"},
          "conditions": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "field": {"type": "string"},
                "op": {"type": "string", "enum": ["eq", "ne", "contains"]},
                "value": {}
              },
              "required": ["field", "op", "value"],
              "additionalProperties": False
            },
            "description": "e.g., [{field:'email', op:'contains', value:'@example.com'}]"
          },
          "logic": {"type": "string", "enum": ["AND", "OR"], "default": "AND"},
          "sort_by": {"type": "string"},
          "sort_dir": {"type": "string", "enum": ["asc", "desc"], "default": "asc"},
          "limit": {"type": "integer", "minimum": 1, "default": 50},
          "offset": {"type": "integer", "minimum": 0, "default": 0}
        },
        "required": ["collection"],
        "additionalProperties": False
      }
    }
  },
  {
    "type": "function",
    "function": {
      "name": "write_to_json",
      "description": "Insert a new item into a collection. If id is missing, it is autogenerated.",
      "parameters": {
        "type": "object",
        "properties": {
          "collection": {"type": "string", "pattern": "^[a-zA-Z0-9_-]+$"},
          "item": {"type": "object"},
          "idempotency_key": {"type": "string", "description": "Optional key to prevent duplicates."}
        },
        "required": ["collection", "item"],
        "additionalProperties": False
      }
    }
  },
  {
    "type": "function",
    "function": {
      "name": "update_in_json",
      "description": "Update an existing item by ID (merge or replace). Optional upsert.",
      "parameters": {
        "type": "object",
        "properties": {
          "collection": {"type": "string", "pattern": "^[a-zA-Z0-9_-]+$"},
          "id": {"oneOf": [{"type":"integer"}, {"type":"string"}]},
          "patch": {"type": "object", "description": "Fields to update for 'merge', or full object for 'replace'."},
          "mode": {"type": "string", "enum": ["merge", "replace"], "default": "merge"},
          "upsert": {"type": "boolean", "default": False},
          "idempotency_key": {"type": "string"}
        },
        "required": ["collection", "id", "patch"],
        "additionalProperties": False
      }
    }
  }
]

# --- Storage helpers ---

def _load_store() -> Dict[str, Any]:
    if not os.path.exists(STORE_PATH):
        return {}
    with open(STORE_PATH, "r", encoding="utf-8") as f:
        return json.load(f)


def _atomic_write(path: str, data: Any):
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    fd, tmp = tempfile.mkstemp(prefix=".tmp_", dir=os.path.dirname(path) or ".")
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        os.replace(tmp, path)
    finally:
        try:
            if os.path.exists(tmp):
                os.remove(tmp)
        except Exception:
            pass


def _save_store(store: Dict[str, Any]):
    _atomic_write(STORE_PATH, store)


def _ensure_collection(store: Dict[str, Any], collection: str) -> List[Dict[str, Any]]:
    if collection not in store or not isinstance(store.get(collection), list):
        store[collection] = []
    return store[collection]


def _coerce_int(v):
    try:
        return int(v)
    except Exception:
        return v


def _get_next_id(items: List[Dict[str, Any]]) -> int:
    max_id = 0
    for it in items:
        if isinstance(it, dict) and "id" in it:
            try:
                val = int(it["id"])
                if val > max_id:
                    max_id = val
            except Exception:
                pass
    return max_id + 1


def _read_dot_path(obj: Any, path: str):
    cur = obj
    for part in path.split("."):
        if isinstance(cur, dict):
            cur = cur.get(part)
        elif isinstance(cur, list):
            try:
                idx = int(part)
                cur = cur[idx]
            except Exception:
                return None
        else:
            return None
        if cur is None:
            return None
    return cur


def _match(item: Dict[str, Any], conds: List[Dict[str, Any]] | None, logic: str) -> bool:
    if not conds:
        return True
    results = []
    for c in conds:
        field, op, value = c.get("field"), c.get("op"), c.get("value")
        v = item
        for part in str(field).split("."):
            if isinstance(v, dict) and part in v:
                v = v[part]
            else:
                v = None
                break
        if op == "eq":
            results.append(v == value)
        elif op == "ne":
            results.append(v != value)
        elif op == "contains":
            try:
                results.append(str(value).lower() in str(v).lower())
            except Exception:
                results.append(False)
        else:
            results.append(False)
    return all(results) if logic == "AND" else any(results)


def _seen_idempotency(key: Optional[str]) -> bool:
    if not key:
        return False
    if not os.path.exists(IDEMPOTENCY_LOG):
        return False
    with open(IDEMPOTENCY_LOG, "r", encoding="utf-8") as f:
        return key.strip() in {line.strip() for line in f.readlines()}


def _mark_idempotency(key: Optional[str]):
    if not key:
        return
    with open(IDEMPOTENCY_LOG, "a", encoding="utf-8") as f:
        f.write(key.strip() + "\n")

# --- Pydantic models (v1/v2 compatible) ---
try:
    # pydantic v2
    from pydantic import BaseModel, Field, ValidationError, ConfigDict
    class BaseForbidModel(BaseModel):
        model_config = ConfigDict(extra='forbid')
    V2 = True
except Exception:
    # pydantic v1 fallback
    from pydantic import BaseModel, Field, ValidationError, Extra
    class BaseForbidModel(BaseModel):
        class Config:
            extra = Extra.forbid
    V2 = False

class Condition(BaseForbidModel):
    field: str
    op: Literal['eq','ne','contains']
    value: Any

class ReadJsonArgs(BaseForbidModel):
    collection: Optional[str] = None
    id: Optional[Any] = None
    path: Optional[str] = None

class FindInJsonArgs(BaseForbidModel):
    collection: str
    conditions: Optional[List[Condition]] = None
    logic: Literal['AND','OR'] = 'AND'
    sort_by: Optional[str] = None
    sort_dir: Literal['asc','desc'] = 'asc'
    limit: int = 50
    offset: int = 0

class WriteToJsonArgs(BaseForbidModel):
    collection: str
    item: Dict[str, Any]
    idempotency_key: Optional[str] = None

class UpdateInJsonArgs(BaseForbidModel):
    collection: str
    id: Any
    patch: Dict[str, Any]
    mode: Literal['merge','replace'] = 'merge'
    upsert: bool = False
    idempotency_key: Optional[str] = None

# Small runtime checks (regex, ranges) kept outside models for v1/v2 simplicity
_COLLECTION_RE = re.compile(r'^[a-zA-Z0-9_-]+$')

def _validate_collection(name: Optional[str]):
    if name is None:
        return
    if not _COLLECTION_RE.fullmatch(name):
        raise ValueError("invalid collection name; allowed [a-zA-Z0-9_-]")

def _normalize_pagination(limit: int, offset: int) -> tuple[int,int]:
    if limit is None or limit < 1:
        limit = 50
    if offset is None or offset < 0:
        offset = 0
    return limit, offset

# --- Tool implementations ---

def read_json(collection: Optional[str] = None, id: Any = None, path: Optional[str] = None):
    store = _load_store()
    if path:
        return {"data": _read_dot_path(store, path)}
    if not collection:
        return {"data": store}
    _validate_collection(collection)
    items = _ensure_collection(store, collection)
    if id is None:
        return {"data": items}
    # get by id (numeric or string)
    rec_id = _coerce_int(id)
    for it in items:
        if _coerce_int(it.get('id')) == rec_id:
            return {"data": it}
    return {"data": None}


def find_in_json(collection: str, conditions: Optional[List[Dict[str, Any]]] = None,
                 logic: str = 'AND', sort_by: Optional[str] = None, sort_dir: str = 'asc',
                 limit: int = 50, offset: int = 0):
    _validate_collection(collection)
    limit, offset = _normalize_pagination(limit, offset)
    store = _load_store()
    items = _ensure_collection(store, collection)
    filtered = [it for it in items if _match(it, conditions or [], logic)]
    if sort_by:
        filtered.sort(key=lambda x: (x or {}).get(sort_by), reverse=(sort_dir == 'desc'))
    total = len(filtered)
    sliced = filtered[offset: offset + limit]
    return {"total": total, "count": len(sliced), "data": sliced}


def write_to_json(collection: str, item: Dict[str, Any], idempotency_key: Optional[str] = None):
    _validate_collection(collection)
    if _seen_idempotency(idempotency_key):
        return {"status": "skipped", "reason": "duplicate_idempotency_key"}
    store = _load_store()
    items = _ensure_collection(store, collection)
    if 'id' not in item:
        item['id'] = _get_next_id(items)
    else:
        # block duplicate IDs
        rec_id = _coerce_int(item['id'])
        for it in items:
            if _coerce_int(it.get('id')) == rec_id:
                return {"error": "duplicate_id", "id": item['id']}
    items.append(item)
    _save_store(store)
    _mark_idempotency(idempotency_key)
    return {"status": "created", "id": item['id'], "data": item}


def update_in_json(collection: str, id: Any, patch: Dict[str, Any],
                   mode: str = 'merge', upsert: bool = False, idempotency_key: Optional[str] = None):
    _validate_collection(collection)
    if _seen_idempotency(idempotency_key):
        return {"status": "skipped", "reason": "duplicate_idempotency_key"}
    store = _load_store()
    items = _ensure_collection(store, collection)
    rec_id = _coerce_int(id)
    record = None
    for it in items:
        if _coerce_int(it.get('id')) == rec_id:
            record = it
            break

    if record is None:
        if not upsert:
            return {"error": "not_found", "id": id}
        new_item = dict(patch)
        new_item['id'] = rec_id
        items.append(new_item)
        _save_store(store)
        _mark_idempotency(idempotency_key)
        return {"status": "upserted", "id": new_item['id'], "data": new_item}

    if mode == 'replace':
        new_obj = dict(patch)
        new_obj['id'] = record.get('id')
        items[items.index(record)] = new_obj
        saved = new_obj
    else:
        record.update({k: v for k, v in patch.items() if k != 'id'})
        saved = record

    _save_store(store)
    _mark_idempotency(idempotency_key)
    return {"status": "updated", "id": saved['id'], "data": saved}

# --- Validation + dispatch ---

VALIDATORS = {
    'read_json': ReadJsonArgs,
    'find_in_json': FindInJsonArgs,
    'write_to_json': WriteToJsonArgs,
    'update_in_json': UpdateInJsonArgs,
}

IMPL = {
    'read_json': read_json,
    'find_in_json': find_in_json,
    'write_to_json': write_to_json,
    'update_in_json': update_in_json,
}


def _schema_for(model_cls: Any) -> Dict[str, Any]:
    try:
        # pydantic v2
        return model_cls.model_json_schema()
    except Exception:
        # pydantic v1
        return model_cls.schema()


def validate_and_dispatch(tool_name: str, args: Dict[str, Any]) -> Dict[str, Any]:
    if tool_name not in VALIDATORS:
        return {"error": "unknown_tool", "name": tool_name}
    model = VALIDATORS[tool_name]
    try:
        parsed = model(**args)
    except ValidationError as ve:
        return {
            "error": "schema_validation_failed",
            "tool": tool_name,
            "issues": ve.errors(),
            "expected": _schema_for(model)
        }

    # Additional lightweight runtime checks
    if tool_name in ("find_in_json",):
        # clamp pagination
        if parsed.limit is None or parsed.limit < 1:
            parsed.limit = 50
        if parsed.offset is None or parsed.offset < 0:
            parsed.offset = 0

    payload = parsed.dict() if hasattr(parsed, 'dict') else parsed.model_dump()  # v1/v2
    # Map to implementation
    fn = IMPL[tool_name]
    try:
        return fn(**payload)
    except Exception as e:
        return {"error": "internal_error", "detail": str(e)}

# --- Tool-call handler for LM Studio ---

def handle_tool_call(tc) -> Dict[str, Any]:
    """Translate LM Studio tool call to local function execution with validation."""
    name = getattr(tc.function, 'name', None)
    args_raw = getattr(tc.function, 'arguments', '{}') or '{}'
    try:
        args = json.loads(args_raw)
    except Exception as e:
        return {"error": "bad_arguments_json", "detail": str(e)}
    return validate_and_dispatch(name, args)

# --- Simple auto-repair chat loop ---

def chat_with_tools(messages: List[Dict[str, Any]], max_repairs: int = 2) -> List[Dict[str, Any]]:
    """Run a chat with tools, letting the model fix bad calls up to `max_repairs` times."""
    repairs = 0
    while True:
        resp = client.chat.completions.create(
            model=MODEL,
            messages=messages,
            tools=tools,
            tool_choice="auto"
        )
        msg = resp.choices[0].message

        if not getattr(msg, 'tool_calls', None):
            # final assistant message (no tool calls)
            messages.append({"role": "assistant", "content": msg.content or ""})
            break

        had_error = False
        for tc in msg.tool_calls:
            result = handle_tool_call(tc)
            if 'error' in result:
                had_error = True
            messages.append({
                "role": "tool",
                "tool_call_id": tc.id,
                "name": tc.function.name,
                "content": json.dumps(result, ensure_ascii=False)
            })

        if not had_error or repairs >= max_repairs:
            # Ask for final NL confirmation/summary
            final = client.chat.completions.create(model=MODEL, messages=messages)
            messages.append({"role": "assistant", "content": final.choices[0].message.content or ""})
            break
        repairs += 1

    return messages

# --- Demo ---
if __name__ == "__main__":
    # Seed store with structure if empty
    if not os.path.exists(STORE_PATH):
        _atomic_write(STORE_PATH, {"customers": []})
        print(f"Initialized {STORE_PATH} with empty 'customers' collection.")

    convo = [
        {"role": "system", "content": "Use tools to manage the JSON store when needed."},
        {"role": "user", "content": "Dodaj kupca Ana (ana@example.com), pa pronađi sve s '@example.com'."}
    ]

    transcript = chat_with_tools(convo, max_repairs=2)
    print("\n--- Conversation Transcript ---")
    for m in transcript:
        role = m.get('role')
        content = m.get('content')
        if role in ("assistant", "tool"):
            print(f"[{role}] {content[:500]}{'...' if content and len(content)>500 else ''}")
        else:
            print(f"[{role}] {content}")


# === GANTT TOOLS (project/position/process bars & milestones) ===
from datetime import datetime, timedelta

# Working calendar defaults (Mon-Fri)
WORKING_DAYS = set(int(x) for x in os.environ.get("WORKING_DAYS", "0,1,2,3,4").split(","))  # 0=Mon ... 6=Sun
HOLIDAYS = set(os.environ.get("HOLIDAYS", "").split(","))  # YYYY-MM-DD, comma-separated

# ---- Date helpers ----
_DEF_FMT = "%Y-%m-%d"

def _dparse(s: str) -> datetime:
    return datetime.strptime(s, _DEF_FMT)

def _dstr(d: datetime) -> str:
    return d.strftime(_DEF_FMT)

def _is_holiday(d: datetime) -> bool:
    return _dstr(d) in HOLIDAYS

def _is_working_day(d: datetime) -> bool:
    return d.weekday() in WORKING_DAYS and not _is_holiday(d)

def _shift_calendar(d: str, delta_days: int) -> str:
    return _dstr(_dparse(d) + timedelta(days=delta_days))

def _shift_workdays(d: str, delta_days: int) -> str:
    # Move N working days forward/backward
    step = 1 if delta_days >= 0 else -1
    count = abs(delta_days)
    cur = _dparse(d)
    while count > 0:
        cur += timedelta(days=step)
        if _is_working_day(cur):
            count -= 1
    return _dstr(cur)

def _snap_to_working_day(d: str, direction: str = "forward") -> str:
    cur = _dparse(d)
    if direction == "auto":
        # keep if working day, else forward
        if _is_working_day(cur):
            return d
        direction = "forward"
    step = 1 if direction == "forward" else -1
    while not _is_working_day(cur):
        cur += timedelta(days=step)
    return _dstr(cur)

# ---- Gantt navigation helpers ----

def _find_project(store: Dict[str, Any], project_id: str) -> Dict[str, Any] | None:
    for pr in store.get("projects", []):
        if pr.get("id") == project_id:
            return pr
    return None


def _find_position(project: Dict[str, Any], position_id: str) -> Dict[str, Any] | None:
    for p in project.get("positions", []):
        if p.get("id") == position_id:
            return p
    return None

# ---- Pydantic models for tools ----
class GanttReadArgs(BaseForbidModel):
    project_id: str
    level: Literal['project','position','process','milestone'] = 'project'
    position_id: Optional[str] = None
    process_name: Optional[str] = None

class GanttUpdateBarArgs(BaseForbidModel):
    project_id: str
    level: Literal['project','position']
    position_id: Optional[str] = None
    start: Optional[str] = None  # YYYY-MM-DD
    end: Optional[str] = None
    snap: Optional[Literal['none','forward','backward','auto']] = 'none'
    idempotency_key: Optional[str] = None

class GanttUpdateProcessArgs(BaseForbidModel):
    project_id: str
    position_id: str
    process_name: str
    planned_start: Optional[str] = None
    planned_end: Optional[str] = None
    snap: Optional[Literal['none','forward','backward','auto']] = 'none'
    idempotency_key: Optional[str] = None

class GanttUpsertMilestoneArgs(BaseForbidModel):
    project_id: str
    position_id: str
    milestone_id: Optional[str] = None  # if missing -> create
    date: str
    title: str
    idempotency_key: Optional[str] = None

class GanttDeleteMilestoneArgs(BaseForbidModel):
    project_id: str
    position_id: str
    milestone_id: str

class GanttShiftArgs(BaseForbidModel):
    project_id: str
    scope: Literal['project','positions','processes','milestones'] = 'project'
    position_ids: Optional[List[str]] = None
    process_names: Optional[List[str]] = None
    delta_days: int
    workdays: bool = True
    cascade: bool = True
    idempotency_key: Optional[str] = None

class GanttSnapArgs(BaseForbidModel):
    project_id: str
    scope: Literal['project','positions','processes','milestones'] = 'project'
    direction: Literal['forward','backward','auto'] = 'auto'
    position_ids: Optional[List[str]] = None
    process_names: Optional[List[str]] = None

class GanttCheckOverlapsArgs(BaseForbidModel):
    project_id: str
    by: Literal['owner','assignee'] = 'owner'  # checks process.owner or task.assignee windows

# ---- Gantt tool implementations ----

def gantt_read(project_id: str, level: str = 'project', position_id: str | None = None, process_name: str | None = None):
    store = _load_store()
    pr = _find_project(store, project_id)
    if not pr:
        return {"error": "project_not_found", "project_id": project_id}
    if level == 'project':
        return {"data": pr.get("gantt")}
    if level == 'position':
        if not position_id:
            return {"error": "position_id_required"}
        pos = _find_position(pr, position_id)
        return {"data": None if not pos else pos.get("gantt")}
    if level == 'process':
        if not position_id or not process_name:
            return {"error": "position_id_and_process_name_required"}
        pos = _find_position(pr, position_id)
        if not pos:
            return {"data": None}
        proc = next((pp for pp in pos.get("processes", []) if pp.get("name") == process_name), None)
        if not proc:
            return {"data": None}
        return {"data": {"plannedStart": proc.get("plannedStart"), "plannedEnd": proc.get("plannedEnd")}}
    if level == 'milestone':
        if not position_id:
            return {"error": "position_id_required"}
        pos = _find_position(pr, position_id)
        return {"data": None if not pos else pos.get("gantt", {}).get("milestones", [])}
    return {"error": "invalid_level"}


def _apply_snap(date_str: str | None, snap: str) -> str | None:
    if not date_str or snap in (None, 'none'):
        return date_str
    return _snap_to_working_day(date_str, 'forward' if snap == 'forward' else 'backward' if snap == 'backward' else 'auto')


def gantt_update_bar(project_id: str, level: str, position_id: str | None = None,
                     start: str | None = None, end: str | None = None,
                     snap: str = 'none', idempotency_key: str | None = None):
    if _seen_idempotency(idempotency_key):
        return {"status": "skipped", "reason": "duplicate_idempotency_key"}
    store = _load_store()
    pr = _find_project(store, project_id)
    if not pr:
        return {"error": "project_not_found", "project_id": project_id}

    def _ensure_order(s, e):
        if s and e and _dparse(s) > _dparse(e):
            return {"error": "invalid_range", "detail": "start after end"}
        return None

    if level == 'project':
        g = pr.setdefault('gantt', {})
        if start: g['start'] = _apply_snap(start, snap)
        if end:   g['end'] = _apply_snap(end, snap)
        err = _ensure_order(g.get('start'), g.get('end'))
        if err: return err
        _save_store(store)
        _mark_idempotency(idempotency_key)
        return {"status": "updated", "data": g}

    if level == 'position':
        if not position_id:
            return {"error": "position_id_required"}
        pos = _find_position(pr, position_id)
        if not pos:
            return {"error": "position_not_found", "position_id": position_id}
        gb = pos.setdefault('gantt', {}).setdefault('bar', {})
        if start: gb['start'] = _apply_snap(start, snap)
        if end:   gb['end'] = _apply_snap(end, snap)
        err = _ensure_order(gb.get('start'), gb.get('end'))
        if err: return err
        _save_store(store)
        _mark_idempotency(idempotency_key)
        return {"status": "updated", "data": pos.get('gantt')}

    return {"error": "invalid_level"}


def gantt_update_process_window(project_id: str, position_id: str, process_name: str,
                                planned_start: str | None = None, planned_end: str | None = None,
                                snap: str = 'none', idempotency_key: str | None = None):
    if _seen_idempotency(idempotency_key):
        return {"status": "skipped", "reason": "duplicate_idempotency_key"}
    store = _load_store()
    pr = _find_project(store, project_id)
    if not pr:
        return {"error": "project_not_found", "project_id": project_id}
    pos = _find_position(pr, position_id)
    if not pos:
        return {"error": "position_not_found", "position_id": position_id}
    proc = next((pp for pp in pos.get("processes", []) if pp.get("name") == process_name), None)
    if not proc:
        return {"error": "process_not_found", "process": process_name}
    if planned_start: proc['plannedStart'] = _apply_snap(planned_start, snap)
    if planned_end:   proc['plannedEnd'] = _apply_snap(planned_end, snap)
    # order check
    if proc.get('plannedStart') and proc.get('plannedEnd') and _dparse(proc['plannedStart']) > _dparse(proc['plannedEnd']):
        return {"error": "invalid_range", "detail": "plannedStart after plannedEnd"}
    _save_store(store)
    _mark_idempotency(idempotency_key)
    return {"status": "updated", "data": {"plannedStart": proc.get('plannedStart'), "plannedEnd": proc.get('plannedEnd')}}


def gantt_upsert_milestone(project_id: str, position_id: str, milestone_id: str | None, title: str, date: str, idempotency_key: str | None = None):
    if _seen_idempotency(idempotency_key):
        return {"status": "skipped", "reason": "duplicate_idempotency_key"}
    store = _load_store()
    pr = _find_project(store, project_id)
    if not pr:
        return {"error": "project_not_found", "project_id": project_id}
    pos = _find_position(pr, position_id)
    if not pos:
        return {"error": "position_not_found", "position_id": position_id}
    gantt = pos.setdefault('gantt', {})
    mlist = gantt.setdefault('milestones', [])
    if milestone_id:
        m = next((m for m in mlist if m.get('id') == milestone_id), None)
        if not m:
            # create with given id
            m = {"id": milestone_id, "date": date, "title": title}
            mlist.append(m)
        else:
            m.update({"date": date, "title": title})
    else:
        mid = f"ms-{int(datetime.utcnow().timestamp())}"
        mlist.append({"id": mid, "date": date, "title": title})
    _save_store(store)
    _mark_idempotency(idempotency_key)
    return {"status": "ok", "milestones": mlist}


def gantt_delete_milestone(project_id: str, position_id: str, milestone_id: str):
    store = _load_store()
    pr = _find_project(store, project_id)
    if not pr:
        return {"error": "project_not_found", "project_id": project_id}
    pos = _find_position(pr, position_id)
    if not pos:
        return {"error": "position_not_found", "position_id": position_id}
    mlist = pos.get('gantt', {}).get('milestones', [])
    before = len(mlist)
    mlist[:] = [m for m in mlist if m.get('id') != milestone_id]
    _save_store(store)
    return {"status": "deleted" if len(mlist) < before else "not_found", "count_delta": before - len(mlist)}


def _shift_fn(workdays: bool):
    return _shift_workdays if workdays else _shift_calendar


def gantt_shift(project_id: str, scope: str = 'project', position_ids: List[str] | None = None, process_names: List[str] | None = None,
                delta_days: int = 0, workdays: bool = True, cascade: bool = True, idempotency_key: str | None = None):
    if _seen_idempotency(idempotency_key):
        return {"status": "skipped", "reason": "duplicate_idempotency_key"}
    if delta_days == 0:
        return {"status": "no_op"}
    shift = _shift_fn(workdays)
    store = _load_store()
    pr = _find_project(store, project_id)
    if not pr:
        return {"error": "project_not_found", "project_id": project_id}

    changed = {"project":0, "positions":0, "processes":0, "milestones":0}

    if scope == 'project' or cascade:
        g = pr.setdefault('gantt', {})
        if g.get('start'): g['start'] = shift(g['start'], delta_days)
        if g.get('end'):   g['end'] = shift(g['end'], delta_days)
        changed['project'] += 1

    # helper filter
    def _pos_iter():
        for p in pr.get('positions', []):
            if position_ids and p.get('id') not in position_ids:
                continue
            yield p

    if scope in ('project','positions','processes','milestones'):
        for pos in _pos_iter():
            if scope in ('project','positions') or cascade:
                gb = pos.get('gantt', {}).get('bar')
                if gb:
                    if gb.get('start'): gb['start'] = shift(gb['start'], delta_days)
                    if gb.get('end'):   gb['end'] = shift(gb['end'], delta_days)
                    changed['positions'] += 1
            if scope in ('project','processes') or cascade:
                for proc in pos.get('processes', []):
                    if process_names and proc.get('name') not in process_names:
                        continue
                    if proc.get('plannedStart'): proc['plannedStart'] = shift(proc['plannedStart'], delta_days)
                    if proc.get('plannedEnd'):   proc['plannedEnd'] = shift(proc['plannedEnd'], delta_days)
                    changed['processes'] += 1
            if scope in ('project','milestones') or cascade:
                for m in pos.get('gantt', {}).get('milestones', []) or []:
                    if m.get('date'):
                        m['date'] = shift(m['date'], delta_days)
                        changed['milestones'] += 1

    _save_store(store)
    _mark_idempotency(idempotency_key)
    return {"status": "shifted", "changed": changed}


def gantt_snap(project_id: str, scope: str = 'project', direction: str = 'auto', position_ids: List[str] | None = None, process_names: List[str] | None = None):
    store = _load_store()
    pr = _find_project(store, project_id)
    if not pr:
        return {"error": "project_not_found", "project_id": project_id}
    snapped = {"project":0, "positions":0, "processes":0, "milestones":0}
    if scope == 'project':
        g = pr.get('gantt') or {}
        if g.get('start'): g['start'] = _snap_to_working_day(g['start'], direction)
        if g.get('end'):   g['end']   = _snap_to_working_day(g['end'], direction)
        snapped['project'] += 1
    for pos in pr.get('positions', []):
        if position_ids and pos.get('id') not in position_ids:
            continue
        if scope in ('project','positions'):
            gb = pos.get('gantt', {}).get('bar')
            if gb:
                if gb.get('start'): gb['start'] = _snap_to_working_day(gb['start'], direction)
                if gb.get('end'):   gb['end']   = _snap_to_working_day(gb['end'], direction)
                snapped['positions'] += 1
        if scope in ('project','processes'):
            for proc in pos.get('processes', []):
                if process_names and proc.get('name') not in process_names:
                    continue
                if proc.get('plannedStart'): proc['plannedStart'] = _snap_to_working_day(proc['plannedStart'], direction)
                if proc.get('plannedEnd'):   proc['plannedEnd']   = _snap_to_working_day(proc['plannedEnd'], direction)
                snapped['processes'] += 1
        if scope in ('project','milestones'):
            for m in pos.get('gantt', {}).get('milestones', []) or []:
                if m.get('date'):
                    m['date'] = _snap_to_working_day(m['date'], direction)
                    snapped['milestones'] += 1
    _save_store(store)
    return {"status": "snapped", "snapped": snapped}


def gantt_check_overlaps(project_id: str, by: str = 'owner'):
    # Check process windows per owner (from processes[].owner) for overlaps
    store = _load_store()
    pr = _find_project(store, project_id)
    if not pr:
        return {"error": "project_not_found", "project_id": project_id}
    buckets: Dict[str, List[Dict[str, Any]]] = {}
    for pos in pr.get('positions', []):
        for proc in pos.get('processes', []):
            s, e = proc.get('plannedStart'), proc.get('plannedEnd')
            if not s or not e:
                continue
            owner = (proc.get('owner') or {}).get('id') if by == 'owner' else None
            if not owner:
                continue
            buckets.setdefault(owner, []).append({
                'position': pos.get('id'),
                'process': proc.get('name'),
                'start': s, 'end': e
            })
    conflicts = []
    for owner, items in buckets.items():
        items.sort(key=lambda x: x['start'])
        for i in range(1, len(items)):
            prev, cur = items[i-1], items[i]
            if _dparse(cur['start']) <= _dparse(prev['end']):
                conflicts.append({'owner': owner, 'a': prev, 'b': cur})
    return {"conflicts": conflicts, "count": len(conflicts)}

# ---- Register tools in OpenAI schema ----

tools_gantt = [
  {
    "type": "function",
    "function": {
      "name": "gantt_read",
      "description": "Read gantt data at project/position/process/milestone level.",
      "parameters": _schema_for(GanttReadArgs)
    }
  },
  {
    "type": "function",
    "function": {
      "name": "gantt_update_bar",
      "description": "Update project or position gantt bar (start/end) with optional snapping.",
      "parameters": _schema_for(GanttUpdateBarArgs)
    }
  },
  {
    "type": "function",
    "function": {
      "name": "gantt_update_process_window",
      "description": "Update planned window for a process in a position.",
      "parameters": _schema_for(GanttUpdateProcessArgs)
    }
  },
  {
    "type": "function",
    "function": {
      "name": "gantt_upsert_milestone",
      "description": "Create or update a milestone in a position gantt.",
      "parameters": _schema_for(GanttUpsertMilestoneArgs)
    }
  },
  {
    "type": "function",
    "function": {
      "name": "gantt_delete_milestone",
      "description": "Delete a milestone by id from a position gantt.",
      "parameters": _schema_for(GanttDeleteMilestoneArgs)
    }
  },
  {
    "type": "function",
    "function": {
      "name": "gantt_shift",
      "description": "Shift gantt dates by N (work)days at various scopes; optional cascade.",
      "parameters": _schema_for(GanttShiftArgs)
    }
  },
  {
    "type": "function",
    "function": {
      "name": "gantt_snap",
      "description": "Snap gantt dates to working days (forward/backward/auto).",
      "parameters": _schema_for(GanttSnapArgs)
    }
  },
  {
    "type": "function",
    "function": {
      "name": "gantt_check_overlaps",
      "description": "Detect owner-based overlaps across process planned windows.",
      "parameters": _schema_for(GanttCheckOverlapsArgs)
    }
  }
]

# Merge tool schemas
ALL_TOOLS = tools + tools_gantt

# Register validators and implementations
VALIDATORS.update({
    'gantt_read': GanttReadArgs,
    'gantt_update_bar': GanttUpdateBarArgs,
    'gantt_update_process_window': GanttUpdateProcessArgs,
    'gantt_upsert_milestone': GanttUpsertMilestoneArgs,
    'gantt_delete_milestone': GanttDeleteMilestoneArgs,
    'gantt_shift': GanttShiftArgs,
    'gantt_snap': GanttSnapArgs,
    'gantt_check_overlaps': GanttCheckOverlapsArgs,
})

IMPL.update({
    'gantt_read': gantt_read,
    'gantt_update_bar': gantt_update_bar,
    'gantt_update_process_window': gantt_update_process_window,
    'gantt_upsert_milestone': gantt_upsert_milestone,
    'gantt_delete_milestone': gantt_delete_milestone,
    'gantt_shift': gantt_shift,
    'gantt_snap': gantt_snap,
    'gantt_check_overlaps': gantt_check_overlaps,
})

# Ensure chat loop uses ALL_TOOLS
# Replace `tools=tools` with `tools=ALL_TOOLS` in calls
# (Kept backward compatible by using ALL_TOOLS directly below.)

def chat_with_tools(messages: List[Dict[str, Any]], max_repairs: int = 2) -> List[Dict[str, Any]]:
    repairs = 0
    while True:
        resp = client.chat.completions.create(
            model=MODEL,
            messages=messages,
            tools=ALL_TOOLS,
            tool_choice="auto"
        )
        msg = resp.choices[0].message

        if not getattr(msg, 'tool_calls', None):
            messages.append({"role": "assistant", "content": msg.content or ""})
            break

        had_error = False
        for tc in msg.tool_calls:
            result = handle_tool_call(tc)
            if 'error' in result:
                had_error = True
            messages.append({
                "role": "tool",
                "tool_call_id": tc.id,
                "name": tc.function.name,
                "content": json.dumps(result, ensure_ascii=False)
            })

        if not had_error or repairs >= max_repairs:
            final = client.chat.completions.create(model=MODEL, messages=messages)
            messages.append({"role": "assistant", "content": final.choices[0].message.content or ""})
            break
        repairs += 1

    return messages
