# Python LLM Server Requirements
# Install with: pip install -r requirements.txt

# Core FastAPI server
fastapi>=0.104.0
uvicorn[standard]>=0.24.0

# PDF processing
pdf2image>=1.17.0
PyPDF2>=3.0.0

# Image processing
Pillow>=10.0.0

# HTTP client
requests>=2.31.0

# LLM server (choose one)
# Option 1: llama-cpp-python (GGUF support, CUDA acceleration)
llama-cpp-python[cuda]>=0.2.11

# Optional: For OpenAI-compatible servers without llama-cpp
# openai>=1.3.0

# Development tools (optional)
# black>=23.0.0
# pytest>=7.4.0
# httpx>=0.25.0  # for testing FastAPI